{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision  # To be able to access standard datasets more easily\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # To plot and display stuff\n",
    "import torch.optim as optim  # Where the optimization modules are\n",
    "import urllib.request\n",
    "from random import randint\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import platform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1.\n",
    "\n",
    "**Modify the code of mnist-assignment.py to train the chosen network using SGD with minibatches of different sizes (starting from size 1). Discuss how the SGD method with various\n",
    "minibatch sizes compares in terms of speed of convergence and final accuracy on test set with\n",
    "GD.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "202c5f0bf4a14180a09fcaca82ac6940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3134b9af70124ab780187737b7a99bfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a09b20687b043cf88fc51d224521c49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c84eae9da314309be14c90bc00cd75b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using torchvision we can conveniently load some datasets\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Extract tensor of data and labels for both the training and the test set\n",
    "x, y = trainset.data.float(), trainset.targets\n",
    "x_test, y_test = testset.data.float(), testset.targets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([60000, 28, 28])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_ds = TensorDataset(x, y)\n",
    "valid_ds = TensorDataset(x_test, y_test)\n",
    "\n",
    "image_size = trainset.data.shape[1] * trainset.data.shape[2]\n",
    "\n",
    "#net = torch.nn.Sequential(\n",
    "#    torch.nn.Linear(image_size, 300),\n",
    "#    torch.nn.Sigmoid(),\n",
    "#    torch.nn.Linear(300, 10),\n",
    "#)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs=1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epoch = 23\n",
    "train_risks = []\n",
    "test_risks = []\n",
    "n_neurons = 300\n",
    "batch_sizes = [1, 3000, 6000, 12000, x.shape[0]]\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(image_size, n_neurons),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(n_neurons, n_neurons),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(n_neurons, n_neurons),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(n_neurons, 10),\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean() * 100\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f'bs={bs}')\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "    valid_accs_per_epoch = []\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = torch.flatten(xb, start_dim=1, end_dim=2)\n",
    "            outputs = model(inputs)\n",
    "            # Define the empirical risk\n",
    "            Risk = loss_func(outputs, yb)\n",
    "            # Make the backward step\n",
    "            Risk.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_accs = []\n",
    "            for xb, yb in valid_dl:\n",
    "                valid_input = torch.flatten(xb, start_dim=1, end_dim=2)\n",
    "                valid_accs.append(accuracy(model(valid_input), yb))\n",
    "        valid_accs_per_epoch.append(np.mean(valid_accs))\n",
    "            # test_prediction = net(torch.flatten(x_test, start_dim=1, end_dim=2))\n",
    "            # test_acc = accuracy(test_prediction, y_test)\n",
    "            # test_accs.append(test_acc)\n",
    "\n",
    "    # train_risk = Risk.item()\n",
    "    # train_risks.append(train_risk)\n",
    "    #\n",
    "    # test_prediction = net(torch.flatten(x_test, start_dim=1, end_dim=2))\n",
    "    # test_risk = loss_func(test_prediction, y_test)\n",
    "    # test_risks.append(test_risk)\n",
    "    plt.plot(np.arange(1,n_epoch+1,1), valid_accs_per_epoch, label=f'bs = {bs}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Excersices 2\n",
    "**Initialize all the weights and all biases to a common value (for instance 0). Try to train\n",
    "the network either with GD or SGD. What happens? Why?**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([300, 784])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "x, y = load_digits(return_X_y=True)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "weights = torch.randn(n_features, n_classes) / np.sqrt(n_features)\n",
    "bias = torch.randn(n_classes) / np.sqrt(n_features)\n",
    "weights.requires_grad_()\n",
    "bias.requires_grad_()\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(image_size, n_neurons),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(n_neurons, 10),\n",
    ")\n",
    "\n",
    "model[0].weight.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The smaller the minibatches, the faster it will converge"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
